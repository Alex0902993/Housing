---
title: "save_data"
output: html_document
date: "2025-03-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}

library(dplyr)
library(ggplot2)
library(janitor)
library(purrr)
library(readxl)
library(rvest)
library(stringr)
library(tidyr) # Treba za fill data

```

## Downlaoding the data

```{r}
url <- "https://is.gd/1vvBAc"

raw_data <- tempfile(fileext = ".xlsx")

download.file(url, raw_data, method = "auto", mode = "wb")

sheets <- excel_sheets(raw_data)

read_clean <- function(..., sheet){
  read_excel(..., sheet = sheet) |>
    mutate(year = sheet)

  raw_data <- map(
    sheets,
    ~read_clean(raw_data,
                skip = 10,
                sheet = .)
  ) |>
    bind_rows() |>
    clean_names()

  raw_data <- raw_data |>
   rename(
     locality = commune,
     n_offers = nombre_doffres,
     average_price_nominal_euros = prix_moyen_annonce_en_courant,
     average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,
     average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant
   ) |>
    mutate(locality = str_trim(locality)) |>
    select(year, locality, n_offers, starts_with("average"))
}




```

The same code but as function:

```{r, eval=TRUE}

get_raw_data <- function(url = "https://is.gd/1vvBAc"){

  raw_data <- tempfile(fileext = ".xlsx")

  download.file(url,
                raw_data,
                mode = "wb")

  sheets <- excel_sheets(raw_data)

  read_clean <- function(..., sheet){
    read_excel(..., sheet = sheet) %>%
      mutate(year = sheet)
  }

  raw_data <- map_dfr(
    sheets,
    ~read_clean(raw_data,
                skip = 10,
                sheet = .)) %>%
    clean_names()

  raw_data %>%
    rename(
      locality = commune,
      n_offers = nombre_doffres,
      average_price_nominal_euros = prix_moyen_annonce_en_courant,
      average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,
      average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant
           ) %>%
    mutate(locality = str_trim(locality)) %>%
    select(year, locality, n_offers, starts_with("average"))

}


```

```{r}
raw_data <- get_raw_data(url = "https://is.gd/1vvBAc")
glimpse(raw_data)
```


We can now continue by explaining what’s wrong with the data and what cleaning steps need to be taken:

We need clean the data: "Luxembourg" is "Luxembourg-ville" in 2010 and 2011,
then "Luxembourg". "Pétange" is also spelt non-consistently, and we also need
to convert columns to the right type. We also directly remove rows where the
locality contains information on the "Source":

```{r}
clean_raw_data <- function(raw_data){
  raw_data %>%
    mutate(locality = ifelse(grepl("Luxembourg-Ville", locality),
                             "Luxembourg",
                             locality),
           locality = ifelse(grepl("P.tange", locality),
                             "Pétange",
                             locality)
           ) %>%
    filter(!grepl("Source", locality)) %>%
    mutate(across(starts_with("average"), as.numeric))
}
```

```{r}
flat_data <- clean_raw_data(raw_data)
head(flat_data)
```

The chunk above explains what we’re doing and why we’re doing it, and so we write a function (based on what we already wrote). Here again, the advantage of having this as a function will make it easier to run on updated data.

We now continue with establishing a list of communes:


We now need to make sure that we got all the communes/localities 
in there. There were mergers in 2011, 2015 and 2018. So we need 
to account for these localities.

We’re now scraping data from Wikipedia of former Luxembourguish communes:

```{r}
get_former_communes <- function(
            url = "https://is.gd/lux_former_communes",
            min_year = 2009,
            table_position = 3
            ){

  read_html(url) %>%
    html_table() %>%
    pluck(table_position) %>%
    clean_names() %>%
    filter(year_dissolved > min_year)
}

```

```{r}
former_communes <- get_former_communes()
head(former_communes)
```

We can scrape current communes:

```{r}
get_current_communes <- function(
                 url = "https://is.gd/lux_communes",
                 table_position = 2
                 ){

  read_html(url) |>
    html_table() |>
    pluck(table_position) |>
    clean_names() |>
    filter(name_2 != "Name") |>
    rename(commune = name_2) |>
    mutate(commune = str_remove(commune, " .$"))

}

```

```{r}
current_communes <- get_current_communes()
head(current_communes)
```

What’s important is that the code doing the actual work is all being wrapped inside functions. I reiterate: this will make reusing, testing and documenting much easier later on. Using the objects former_communes and current_communes we can now build the complete list:

Let’s now create a list of all communes:

```{r}
get_test_communes <- function(former_communes, current_communes){

  communes <- unique(c(former_communes$name, current_communes$commune))
  # we need to rename some communes

  # Different spelling of these communes between wikipedia and the data

  communes[which(communes == "Clemency")] <- "Clémency"
  communes[which(communes == "Redange")] <- "Redange-sur-Attert"
  communes[which(communes == "Erpeldange-sur-Sûre")] <- "Erpeldange"
  communes[which(communes == "Luxembourg City")] <- "Luxembourg"
  communes[which(communes == "Käerjeng")] <- "Kaerjeng"
  communes[which(communes == "Petange")] <- "Pétange"

  communes
}

```

```{r}
former_communes <- get_former_communes()
current_communes <- get_current_communes()

communes <- get_test_communes(former_communes, current_communes)
head(communes)
```

Once again, we write a function for this. We need to merge these two lists, and need to make sure that the spelling of the communes’ names is unified between this list and between the communes’ names in the data.

We now run the actual test:

Let’s test to see if all the communes from our dataset are represented.

```{r}
setdiff(flat_data$locality, communes)
```

If the above code doesn’t show any communes, then this means that we are
accounting for every commune.


But, I have *[1] NA                  "Moyenne nationale" "Total d'offres"  *!!

This test is quite simple, and we will see how to create something a bit more robust and useful later on.

Now, let’s extract the national average from the data and create a separate dataset with the national level data:

Let’s keep the national average in another dataset:

```{r}

make_country_level_data <- function(flat_data){
  country_level <- flat_data %>%
    filter(grepl("nationale", locality)) %>%
    select(-n_offers)

  offers_country <- flat_data %>%
    filter(grepl("Total d.offres", locality)) %>%
    select(year, n_offers)

  full_join(country_level, offers_country) %>%
    select(year, locality, n_offers, everything()) %>%
    mutate(locality = "Grand-Duchy of Luxembourg")

}

```

```{r}
country_level_data <- make_country_level_data(flat_data)
head(country_level_data)
```

and finally, let’s do the same but for the commune level data:

We can finish cleaning the commune data:

```{r}
make_commune_level_data <- function(flat_data){
  flat_data %>%
    filter(!grepl("nationale|offres", locality),
           !is.na(locality))
}

```

```{r}
commune_level_data <- make_commune_level_data(flat_data)
glimpse(commune_level_data)
```

We can finish with a chunk to save the data to disk:

We now save the dataset in a folder for further analysis (keep chunk option to
`eval = FALSE` to avoid running it when knitting):

```{r, eval = FALSE}
write.csv(commune_level_data,
          "datasets/house_prices_commune_level_data.csv",
          row.names = FALSE)
write.csv(country_level_data,
          "datasets/house_prices_country_level_data.csv",
          row.names = FALSE)
```

Ok, and that’s it. You can take a look at the finalised file here1. You can now remove the save_data.R script, as you have successfully ported the code over to an RMarkdown file. If you have not done it yet, you can commit these changes and push.

Let’s now do the same thing for the analysis script.